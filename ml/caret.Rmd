---
title: "Caret"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```

## The caret package

- We have already learned about regression and kNN as machine learning algorithms.

- In later sections, we learn several others, and this is just a small subset of all the algorithms out there.

- Many of these algorithms are implemented in R.

- However, they are distributed via different packages, developed by different authors, and often use different syntax.

- The __caret__ package tries to consolidate these differences and provide consistency.



## The caret package

- It currently includes 237 different methods which are summarized in the __caret__ [package manual](https://topepo.github.io/caret/available-models.html).

- Keep in mind that __caret__ does not include the needed packages and, to implement a package through __caret__, you still need to install the library.

- The required packages for each method are described in the package manual.

- The __caret__ package also provides a function that performs cross validation for us.



## The caret package

- Here we provide some examples showing how we use this incredibly helpful package.

- We will use the 2 or 7 example to illustrate:

```{r, message=FALSE, warning=FALSE} 
library(tidyverse) 
library(dslabs) 
data("mnist_27") 
``` 



## The caret `train` functon

- The __caret__ `train` function lets us train different algorithms using similar syntax.

- So, for example, we can type:

```{r, warning=FALSE, message=FALSE} 
library(caret) 
train_glm <- train(y ~ ., method = "glm", data = mnist_27$train) 
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train) 
``` 

- To make predictions, we can use the output of this function directly without needing to look at the specifics of `predict.glm` and `predict.knn`.

- Instead, we can learn how to obtain predictions from `predict.train`.



## The caret `train` functon

- The code looks the same for both methods:

```{r} 
y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw") 
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw") 
``` 

- This permits us to quickly compare the algorithms.

- For example, we can compare the accuracy like this:

```{r} 
confusionMatrix(y_hat_glm, mnist_27$test$y)$overall[["Accuracy"]] 
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[["Accuracy"]] 
``` 



## Cross validation

- When an algorithm includes a tuning parameter, `train` automatically uses cross validation to decide among a few default values.

- To find out what parameter or parameters are optimized, you can read [the manual](http://topepo.github.io/caret/available-models.html) or study the output of:

```{r, eval=FALSE} 
getModelInfo("knn") 
``` 

- We can also use a quick lookup like this:

```{r, eval=FALSE} 
modelLookup("knn") 
``` 

## Cross validation

- If we run it with default values:




```{r} 
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train) 
``` 

- you can quickly see the results of the cross validation using the `ggplot` function.

- The argument `highlight` highlights the max:

```{r caret-highlight, eval=FALSE} 
ggplot(train_knn, highlight = TRUE) 
``` 


## Cross validation

```{r caret-highlight-run, echo=FALSE} 
ggplot(train_knn, highlight = TRUE) 
``` 


## Cross validation

- By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations.

- For the `kNN` method, the default is to try $k=5,7,9$.

- We change this using the `tuneGrid` parameter.

- The grid of values must be supplied by a data frame with the parameter names as specified in the `modelLookup` output.

- Here, we present an example where we try out 30 values between 9 and 67.



## Cross validation

- To do this with __caret__, we need to define a column named `k`, so we use this:

- `data.frame(k = seq(9, 67, 2))`.

- Note that when running this code, we are fitting 30 versions of kNN to 25 bootstrapped samples.

- Since we are fitting $30 \times 25 = 750$ kNN models, running this code will take several seconds.

## Cross validation

- We set the seed because cross validation is a random procedure and we want to make sure the result here is reproducible.

```{r train-knn-plot, eval=FALSE} 
set.seed(2008) 
train_knn <- train(y ~ ., method = "knn",  
                   data = mnist_27$train, 
                   tuneGrid = data.frame(k = seq(9, 71, 2))) 
ggplot(train_knn, highlight = TRUE) 
``` 


## Cross validation

```{r train-knn-plot-run, echo=FALSE} 
set.seed(2008) 
train_knn <- train(y ~ ., method = "knn",  
                   data = mnist_27$train, 
                   tuneGrid = data.frame(k = seq(9, 71, 2))) 
ggplot(train_knn, highlight = TRUE) 
``` 


## Cross validation

- To access the parameter that maximized the accuracy, you can use this:

```{r} 
train_knn$bestTune 
``` 

- and the best performing model like this:

```{r} 
train_knn$finalModel 
``` 

## Cross validation

- The function `predict` will use this best performing model.

- Here is the accuracy of the best model when applied to the test set, which we have not used at all yet because the cross validation was done on the training set:


```{r} 
confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"), 
                mnist_27$test$y)$overall["Accuracy"] 
``` 

## Cross validation

- If we want to change how we perform cross validation, we can use the `trainControl` function.

- We can make the code above go a bit faster by using, for example, 10-fold cross validation.

- This means we have 10 samples using 10% of the observations each.

- We accomplish this using the following code:

```{r cv-10-fold-accuracy-estimate, eval=FALSE} 
control <- trainControl(method = "cv", number = 10, p = .9) 
train_knn_cv <- train(y ~ ., method = "knn",  
                   data = mnist_27$train, 
                   tuneGrid = data.frame(k = seq(9, 71, 2)), 
                   trControl = control) 
ggplot(train_knn_cv, highlight = TRUE) 
``` 


## Cross validation

```{r cv-10-fold-accuracy-estimate-run, echo=FALSE} 
control <- trainControl(method = "cv", number = 10, p = .9) 
train_knn_cv <- train(y ~ ., method = "knn",  
                   data = mnist_27$train, 
                   tuneGrid = data.frame(k = seq(9, 71, 2)), 
                   trControl = control) 
ggplot(train_knn_cv, highlight = TRUE) 
``` 


## Cross validation

- We notice that the accuracy estimates are more variable, which is expected since we changed the number of samples used to estimate accuracy.

- Note that `results` component of the `train` output includes several summary statistics related to the variability of the cross validation estimates:

```{r} 
names(train_knn$results) 
``` 

```{r accuracy-with-sd-bars, eval=FALSE} 
train_knn$results |>  
  ggplot(aes(x = k, y = Accuracy)) + 
  geom_line() + 
  geom_point() + 
  geom_errorbar(aes(x = k,  
                    ymin = Accuracy - AccuracySD,  
                    ymax = Accuracy + AccuracySD)) 
``` 


## Cross validation

```{r accuracy-with-sd-bars-run, echo=FALSE} 
train_knn$results |>  
  ggplot(aes(x = k, y = Accuracy)) + 
  geom_line() + 
  geom_point() + 
  geom_errorbar(aes(x = k,  
                    ymin = Accuracy - AccuracySD,  
                    ymax = Accuracy + AccuracySD)) 
``` 



```{r, echo=FALSE} 
plot_cond_prob <- function(p_hat=NULL){ 
  tmp <- mnist_27$true_p 
  if(!is.null(p_hat)){ 
    tmp <- mutate(tmp, p=p_hat) 
  } 
  tmp |> ggplot(aes(x_1, x_2, z=p, fill=p)) + 
  geom_raster(show.legend = FALSE) + 
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  stat_contour(breaks=c(0.5),color="black") 
} 
``` 


## Example: fitting with loess

- The best fitting kNN model approximates the true conditional probability:


```{r mnist27-optimal-knn-fit, echo=FALSE} 
plot_cond_prob(predict(train_knn, mnist_27$true_p, type = "prob")[,2]) 
``` 


## Example: fitting with loess

- However, we do see that the boundary is somewhat wiggly.

- This is because kNN, like the basic bin smoother, does not use a kernel.

- To improve this we could try loess.

- By reading through the available models part of [the manual](https://topepo.github.io/caret/available-models.html) we see that we can use the `gamLoess` method.

## Example: fitting with loess

- In [the manual](https://topepo.github.io/caret/train-models-by-tag.html) we also see that we need to install the __gam__ package if we have not done so already:


```{r, eval=FALSE} 
install.packages("gam") 
``` 

- Then we see that we have two parameters to optimize:

```{r} 
modelLookup("gamLoess") 
``` 

- We will stick to a degree of 1.

## Example: fitting with loess

- But to try out different values for the span, we still have to include a column in the table with the name `degree` so we can do this:

```{r} 
grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1) 
``` 

- We will use the default cross validation control parameters.

```{r loess-accuracy, warning=FALSE, message=FALSE, eval=FALSE} 
train_loess <- train(y ~ .,  
                   method = "gamLoess",  
                   tuneGrid=grid, 
                   data = mnist_27$train) 
ggplot(train_loess, highlight = TRUE) 
``` 


## Example: fitting with loess

```{r loess-accuracy-run, warning=FALSE, message=FALSE, echo=FALSE} 
train_loess <- train(y ~ .,  
                   method = "gamLoess",  
                   tuneGrid=grid, 
                   data = mnist_27$train) 
ggplot(train_loess, highlight = TRUE) 
``` 


## Example: fitting with loess

- We can see that the method performs similar to kNN:

```{r} 
confusionMatrix(data = predict(train_loess, mnist_27$test),  
                reference = mnist_27$test$y)$overall["Accuracy"] 
``` 




## Example: fitting with loess

- and produces a smoother estimate:

```{r gam-smooth, warning=FALSE, echo=FALSE, out.width="100%"} 
p1 <- plot_cond_prob() + ggtitle("True conditional probability") 
p2 <- plot_cond_prob(predict(train_loess, mnist_27$true_p, type = "prob")[,2]) +  
  ggtitle("GAM Loess estimate") 
gridExtra::grid.arrange(p2, p1, nrow=1) 
``` 



